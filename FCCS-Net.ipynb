{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"input_shape = (3, 256, 256)\nnum_classes=2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(input_shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport random\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\n# load libraries\nimport os\n####*IMPORANT*: Have to do this line *before* importing tensorflow\nos.environ['PYTHONHASHSEED']=str(1)\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(1)\n   tf.random.set_seed(1)\n   np.random.seed(1)\n   random.seed(1)\n    \n    \nreset_random_seeds()\n\nfrom tensorflow import keras\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras import models\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Flatten, Input, Dense\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Flatten, Input, Dense","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\n\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\n\nfrom PIL import Image\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.optim as optim\n\n\n# Define a transformation for data preprocessing\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data = []  # List to store data samples\n        self.targets = []  # List to store corresponding labels\n        self.class_to_int = {}  # Dictionary to map class labels to integers\n        self.int_to_class = {}  # Dictionary to map integers to class labels\n        class_index = 0\n        \n        # Iterate over each class in the directory\n        for class_label in os.listdir(data_dir):\n            print(class_label)\n            class_dir = os.path.join(data_dir, class_label)\n            \n            # Add class_label to class_to_int dictionary\n            self.class_to_int[class_label] = class_index\n            self.int_to_class[class_index] = class_label\n            class_index += 1\n            \n            # Iterate over each image in the class directory\n            for image_file in os.listdir(class_dir):\n                image_path = os.path.join(class_dir, image_file)\n                \n                # Add the image path and corresponding label to the lists\n                self.data.append(image_path)\n                self.targets.append(self.class_to_int[class_label])\n        \n        self.transform = transform\n\n        # Print the shape of data and targets\n        print(f\"Data shape: {len(self.data)}\")\n        print(f\"Targets shape: {len(self.targets)}\")\n\n    def __getitem__(self, index):\n        # Retrieve the data sample and its label based on the index\n        data_sample = self.data[index]\n        target = self.targets[index]\n        \n        # Load the image using PIL\n        image = Image.open(data_sample)\n        \n        # Resize the image to 32x32\n        image = image.resize((input_shape[1], input_shape[2]))\n        \n        # Apply the data transformation if specified\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\n    def __len__(self):\n        # Return the total number of samples in the dataset\n        return len(self.data)\n\n\n# Set the path to your custom dataset directory\ndata_dir = '/kaggle/input/breakhis-all-1/BreaKhis_arranged/breast_200/'\n\n# Define the data transformation\ntransform = ToTensor()  # Use ToTensor() to convert the samples to PyTorch tensors\n\n# Create an instance of your custom dataset\ndataset = CustomDataset(data_dir, transform=transform)\n\n# Split the dataset into training, validation, and test sets\ntrain_ratio = 0.7\nval_ratio = 0.1\ntest_ratio = 0.2\n\n# Calculate the sizes of each split\nnum_samples = len(dataset)\ntrain_size = int(train_ratio * num_samples)\nval_size = int(val_ratio * num_samples)\ntest_size = num_samples - train_size - val_size\n\n# Use the train_test_split function to perform the split\ntrain_data, temp_data = train_test_split(dataset, test_size=(val_size + test_size), random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=test_size, random_state=42)\n\n##################\n# Create DataLoader instances for each split\nbatch_size = 128\ntrainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\nvalloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# Print the sizes of each split\nprint(\"Train data size:\", len(train_data))\nprint(\"Validation data size:\", len(val_data))\nprint(\"Test data size:\", len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with residual connection\nnum_classes=2\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Define the CBAM module\nclass CBAM(nn.Module):\n    def __init__(self, in_channels):\n        super(CBAM, self).__init__()\n        self.channel_att = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // 16, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 16, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_att = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            #nn.MaxPool2d(1),\n            nn.Conv2d(in_channels, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x_channel_att = self.channel_att(x)\n        x_spatial_att = self.spatial_att(x)\n        x_att = torch.mul(x_channel_att, x_spatial_att)\n        return torch.mul(x, x_att)\n\n#CBAM with residual connection\nclass ResNet18_CBAM(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18_CBAM, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n\n        # Disable gradients for all the parameters in the pre-trained ResNet18\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        # Enable gradients for the last two layers of ResNet18\n        for param in self.resnet.layer4.parameters():\n            param.requires_grad = False\n        for param in self.resnet.layer3.parameters():\n            param.requires_grad = True\n            \n        self.cbam1 = CBAM(64)  # Apply CBAM to the output of layer3\n        self.cbam2 = CBAM(128)  # Apply CBAM to the output of layer4\n        self.cbam3 = CBAM(256)  # Apply CBAM to the output of layer3\n        self.cbam4 = CBAM(512)  # Apply CBAM to the output of layer4\n\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc1 = nn.Linear(960, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n\n        x_res1 = self.resnet.layer1(x)\n        x_cbam1 = self.cbam1(x_res1)\n        x1 = x_res1 + x_cbam1\n\n        x_res2 = self.resnet.layer2(x1)\n        x_cbam2 = self.cbam2(x_res2)\n        x2 = x_res2 + x_cbam2\n\n        x_res3 = self.resnet.layer3(x2)\n        x_cbam3 = self.cbam3(x_res3)\n        x3 = x_res3 + x_cbam3\n\n        x_res4 = self.resnet.layer4(x3)\n        x_cbam4 = self.cbam4(x_res4)\n        x4 = x_res4 + x_cbam4\n\n        x_cbam1_gap = self.global_avg_pooling(x_cbam1)\n        x_cbam2_gap = self.global_avg_pooling(x_cbam2)\n        x_cbam3_gap = self.global_avg_pooling(x_cbam3)\n        x_cbam4_gap = self.global_avg_pooling(x_cbam4)\n\n        x_cbam_concat = torch.cat([x_cbam1_gap, x_cbam2_gap, x_cbam3_gap, x_cbam4_gap], dim=1)\n        x_cbam_concat = x_cbam_concat.view(x_cbam_concat.size(0), -1)\n        out = self.fc1(x_cbam_concat)\n        out = self.fc2(out)\n        return out\n\n\nimport time\n# Define your model, loss function, and optimizer\nmodel = ResNet18_CBAM(num_classes)  # Replace `ResNet18_CBAM` with your actual model class\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training loop\nnum_epochs = 30 # Adjust the number of epochs as needed\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0.0\n    correct = 0\n    total = 0\n    \n    start_time = time.time()\n\n    for images, labels in trainloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Compute loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    # Calculate average training loss and accuracy\n    train_loss /= len(train_data)\n    train_losses.append(train_loss)\n    train_accuracy = correct / total * 100.0\n    train_accuracies.append(train_accuracy)\n    end_time = time.time()\n    test_time = end_time - start_time\n    print(f\"Training Time: {test_time:.2f} seconds\")\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in valloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Compute loss\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            \n            # Calculate accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss and accuracy\n    val_loss /= len(val_data)\n    val_losses.append(val_loss)\n    val_accuracy = correct / total * 100.0\n    val_accuracies.append(val_accuracy)\n    \n    # Print epoch statistics\n    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, \"\n          f\"Val Accuracy: {val_accuracy:.2f}%\")\n\n# After training, you can evaluate the model on the test data using a similar approach\ntorch.save(model.state_dict(), \"resnet_cbam_weights.pth\")\nimport time\n\n# Evaluate the model on the test set\nmodel.eval()\nmodel = model.to(device)\ncorrect_test = 0\ntotal_test = 0\n\nstart_time = time.time()\n\nwith torch.no_grad():\n    for inputs, labels in testloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted_test = torch.max(outputs.data, 1)\n        total_test += labels.size(0)\n        correct_test += (predicted_test == labels).sum().item()\n\n# Calculate test accuracy for the current epoch\ntest_accuracy = 100 * correct_test / total_test\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# After training, you can evaluate the model on the test data using a similar approach\nmodel.eval()\nall_probs = []\nall_labels = []\nall_preds = []\nwith torch.no_grad():\n    for images, labels in testloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Calculate probabilities\n        probs = torch.softmax(outputs, dim=1)\n        all_probs.append(probs.cpu().numpy())\n        \n        # Calculate predicted labels\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.append(preds.cpu().numpy())\n        \n        all_labels.append(labels.cpu().numpy())\n\nall_probs = np.concatenate(all_probs)\nall_labels = np.concatenate(all_labels)\nall_preds = np.concatenate(all_preds)\n\n# Plot confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(all_labels, all_preds)\n# Calculate overall accuracy from the confusion matrix\noverall_accuracy = np.trace(cm) / np.sum(cm) * 100.0\n\n# Print confusion matrix and overall accuracy\nclass_labels = [\"Ben.\", \"Mal.\"]  # Replace with your class labels\n\n# Plot confusion matrix\nplt.figure(figsize=(4, 3), dpi=300)\nsns.heatmap(cm, annot=True, fmt='d', cmap='viridis', annot_kws={\"ha\": 'center', \"va\": 'center'})  # Set ha and va parameters\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (Overall Accuracy: {overall_accuracy:.2f}%)')\nplt.axhline(y=0, color='k', linewidth=2)  # Add horizontal boundary line\nplt.axhline(y=cm.shape[0], color='k', linewidth=2)\nplt.axvline(x=0, color='k', linewidth=2)  # Add vertical boundary line\nplt.axvline(x=cm.shape[1], color='k', linewidth=2)\nplt.xticks(np.arange(len(class_labels)) + 0.5, class_labels, rotation=0)  # Add class labels to x-axis\nplt.yticks(np.arange(len(class_labels)) + 0.5, class_labels, rotation=90)   # Add class labels to y-axis\n\n# Add class labels in the center of each cell\nfor i in range(len(class_labels)):\n    for j in range(len(class_labels)):\n        plt.text(j + 0.5, i + 0.5, str(cm[i, j]), va='center', ha='center', color='red')\n\n\nplt.show()\n\nprint(f'Overall Accuracy: {overall_accuracy:.2f}%')\n\n\n# Plot class-wise ROC curves\nfrom sklearn.metrics import roc_curve, auc\n\n# Calculate class-wise ROC curves and AUCs\nfpr = {}\ntpr = {}\nroc_auc = {}\n\nfor i in range(num_classes):  # Change `num_classes` to the number of classes in your dataset\n    fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\n# Plot ROC curves for each class\nplt.figure(figsize=(4, 3),dpi=300)\nclass_labels = [\"Benign\", \"Malignant\"]  # Replace with your actual class labels\n\nfor i in range(num_classes):  # Change `num_classes` to the number of classes in your dataset\n    plt.plot(fpr[i], tpr[i], lw=4, label=f' {class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n\n#plt.plot([0, 1], [0, 1], color='navy', lw=4, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Class-wise Receiver Operating Characteristic (ROC) Curves')\nplt.legend(loc='lower right')\nplt.show()\nfrom sklearn.metrics import classification_report\n\n# Calculate class-wise precision, recall, F1-score, and support\ntarget_names = [f'Class {i}' for i in range(num_classes)]  # Change `num_classes` to the number of classes in your dataset\nclass_report = classification_report(all_labels, all_preds, target_names=target_names, digits=4)\n\n# Print the class-wise performance metrics\nprint(\"Class-wise Performance Metrics:\")\nprint(class_report)\nepochs = range(1, num_epochs + 1)\nplt.figure(figsize=(4, 3), dpi=300)  # Set figure size and dpi\nplt.plot(epochs, train_accuracies, label='Training Accuracy', linewidth=2, color='red')\nplt.plot(epochs, val_accuracies, label='Val Accuracy', linewidth=2, color='green')\nplt.title('Training and Test Accuracy Curves')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend(fontsize=10)  # Set legend fontsize\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# ... (other code)\n\n# Create an instance of your custom dataset\ndataset = CustomDataset(data_dir, transform=transform)\n\n# Get the class labels and their corresponding counts\nclass_labels = dataset.class_to_int.keys()\nclass_counts = {class_label: 0 for class_label in class_labels}\n\n# Count the number of samples in each class\nfor target in dataset.targets:\n    class_counts[dataset.int_to_class[target]] += 1\n\n# Print the class names and their corresponding sample counts\nfor class_label, count in class_counts.items():\n    print(f\"Class: {class_label}, Number of Samples: {count}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize lists to store the data and labels\nx_train = []\ny_train = []\nx_val = []\ny_val = []\nx_test = []\ny_test = []\n\n# Iterate over the trainloader to get the training data and labels\nfor images, labels in trainloader:\n    x_train.append(images)\n    y_train.append(labels)\n\n# Iterate over the valloader to get the validation data and labels\nfor images, labels in valloader:\n    x_val.append(images)\n    y_val.append(labels)\n\n# Iterate over the testloader to get the test data and labels\nfor images, labels in testloader:\n    x_test.append(images)\n    y_test.append(labels)\n\n# Concatenate the lists to create a single tensor for each dataset\nx_train = torch.cat(x_train)\ny_train = torch.cat(y_train)\nx_val = torch.cat(x_val)\ny_val = torch.cat(y_val)\nx_test = torch.cat(x_test)\ny_test = torch.cat(y_test)\n\n# Print the shapes of the datasets\nprint(\"x_train shape:\", x_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"x_val shape:\", x_val.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"x_test shape:\", x_test.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#CBAM original\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Define the CBAM module\nclass CBAM(nn.Module):\n    def __init__(self, in_channels):\n        super(CBAM, self).__init__()\n        self.channel_att = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // 16, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 16, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_att = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1)\n            nn.Conv2d(in_channels, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x_channel_att = self.channel_att(x)\n        print(\"x_channel_att\",x_channel_att.shape)\n        x_spatial_att = self.spatial_att(x)\n        print(\"x_spatial_att\",x_spatial_att.shape )\n        x_att = torch.mul(x_channel_att, x_spatial_att)\n        print(\"x_att\",x_att.shape )\n        return torch.mul(x, x_att)\n\n# Define the ResNet18 model with CBAM and GAP\nclass ResNet18_CBAM(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18_CBAM, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        \n        # Disable gradients for all the parameters in the pre-trained ResNet18\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n            \n        # Enable gradients for the last two layers of ResNet18\n        for param in self.resnet.layer4.parameters():\n            param.requires_grad = False\n        for param in self.resnet.layer3.parameters():\n            param.requires_grad = True\n        \n        self.cbam1 = CBAM(64)  # Apply CBAM to the output of layer3\n        self.cbam2 = CBAM(128)  # Apply CBAM to the output of layer4\n        self.cbam3 = CBAM(256)  # Apply CBAM to the output of layer3\n        self.cbam4 = CBAM(512)  # Apply CBAM to the output of layer4\n        \n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc1 = nn.Linear(960, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        print(\"conv1:\", x.shape)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n        print(\"maxpool:\", x.shape)\n        \n        x = self.resnet.layer1(x)\n        x_cbam1 = self.cbam1(x)\n        print(\"layer1:\", x.shape)\n        x = self.resnet.layer2(x)\n        print(\"layer2:\", x.shape)\n        x_cbam2 = self.cbam2(x)  # Apply CBAM to layer2 output\n        print(\"cbam2:\", x.shape)\n        x = self.resnet.layer3(x)##256\n        print(\"layer3:\", x.shape)\n        x_cbam3 = self.cbam3(x)  # Apply CBAM to layer3 output\n        print(\" cbam3:\", x.shape)\n        x = self.resnet.layer4(x) ##512\n        print(\" layer4:\", x.shape)\n        x_cbam4 = self.cbam4(x)\n        print(\" cbam4:\", x_cbam4.shape)\n        x_cbam1_gap = self.global_avg_pooling(x_cbam1)\n        print(\" GAP cbam1:\",  x_cbam1_gap.shape)\n        x_cbam2_gap = self.global_avg_pooling(x_cbam2)\n        print(\" GAP cbam2:\",  x_cbam2_gap.shape)\n        x_cbam3_gap = self.global_avg_pooling(x_cbam3)\n        print(\" GAP cbam3:\",  x_cbam3_gap.shape)\n        x_cbam4_gap = self.global_avg_pooling(x_cbam4)\n        print(\" GAP cbam4:\",  x_cbam4_gap.shape)\n        \n        x_cbam_concat = torch.cat([x_cbam1_gap,x_cbam2_gap,x_cbam3_gap,x_cbam4_gap], dim=1)\n        print(\" cbam concat:\",  x_cbam_concat.shape )\n        x_cbam_concat = x_cbam_concat.view(x_cbam_concat.size(0), -1)\n        #print(\"Shape after after view:\",  x_cbam_concat.shape )\n        out = self.fc1(x_cbam_concat)\n        print(\"out_fc1\",out.shape)\n        out=self.fc2(out)\n        print(\"out_fc2\",out.shape)\n        #print(\"Shape fc:\",  out.shape )\n        return out\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#with residual connection\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Define the CBAM module\nclass CBAM(nn.Module):\n    def __init__(self, in_channels):\n        super(CBAM, self).__init__()\n        self.channel_att = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // 16, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 16, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_att = nn.Sequential(\n            nn.Conv2d(in_channels, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x_channel_att = self.channel_att(x)\n        x_spatial_att = self.spatial_att(x)\n        x_att = torch.mul(x_channel_att, x_spatial_att)\n        return x_spatial_att #torch.mul(x, x_att)\n\n#CBAM with residual connection\nclass ResNet18_CBAM(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18_CBAM, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n\n        # Disable gradients for all the parameters in the pre-trained ResNet18\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        # Enable gradients for the last two layers of ResNet18\n        for param in self.resnet.layer4.parameters():\n            param.requires_grad = False\n        for param in self.resnet.layer3.parameters():\n            param.requires_grad = True\n            \n        self.cbam1 = CBAM(64)  # Apply CBAM to the output of layer3\n        self.cbam2 = CBAM(128)  # Apply CBAM to the output of layer4\n        self.cbam3 = CBAM(256)  # Apply CBAM to the output of layer3\n        self.cbam4 = CBAM(512)  # Apply CBAM to the output of layer4\n\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc1 = nn.Linear(960, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n\n        x_res1 = self.resnet.layer1(x)\n        x_cbam1 = self.cbam1(x_res1)\n        x1 = x_res1 + x_cbam1\n\n        x_res2 = self.resnet.layer2(x1)\n        x_cbam2 = self.cbam2(x_res2)\n        x2 = x_res2 + x_cbam2\n\n        x_res3 = self.resnet.layer3(x2)\n        x_cbam3 = self.cbam3(x_res3)\n        x3 = x_res3 + x_cbam3\n\n        x_res4 = self.resnet.layer4(x3)\n        x_cbam4 = self.cbam4(x_res4)\n        x4 = x_res4 + x_cbam4\n\n        x_cbam1_gap = self.global_avg_pooling(x1)\n        x_cbam2_gap = self.global_avg_pooling(x2)\n        x_cbam3_gap = self.global_avg_pooling(x3)\n        x_cbam4_gap = self.global_avg_pooling(x4)\n\n        x_cbam_concat = torch.cat([x_cbam1_gap, x_cbam2_gap, x_cbam3_gap, x_cbam4_gap], dim=1)\n        x_cbam_concat = x_cbam_concat.view(x_cbam_concat.size(0), -1)\n        out = self.fc1(x_cbam_concat)\n        out = self.fc2(out)\n        return out\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\ninput_shape = (3, 256, 256)  # Example input shape for RGB images\nnum_classes = 2  # Example number of classes\nmodel = ResNet18_CBAM(num_classes)\n\n# Create a random image tensor\nimage = torch.randn(1, *input_shape)\n\n# Pass the image through the model\noutput = model(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the ResNet18_CBAM model\n#model = ResNet18_CBAM(num_classes)\n\n# Count the number of trainable and non-trainable parameters\nnum_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nnum_non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n\n# Print the number of trainable and non-trainable parameters\nprint(\"Number of trainable parameters: \", num_trainable_params)\nprint(\"Number of non-trainable parameters: \", num_non_trainable_params)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models import resnet18\nfrom torchsummary import summary\n\n# Instantiate the ResNet18 model from torchvision\nmodel = ResNet18_CBAM(num_classes)\n\n# Print the model summary\nsummary(model, (3, 256, 256))\n\n# Print the names of trainable and non-trainable layers\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(\"Trainable layer:\", name)\n    else:\n        print(\"Non-trainable layer:\", name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n# Define your model, loss function, and optimizer\nmodel = ResNet18_CBAM(num_classes)  # Replace `ResNet18_CBAM` with your actual model class\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training loop\nnum_epochs = 10 # Adjust the number of epochs as needed\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0.0\n    correct = 0\n    total = 0\n    \n    start_time = time.time()\n\n    for images, labels in trainloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Compute loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    # Calculate average training loss and accuracy\n    train_loss /= len(train_data)\n    train_losses.append(train_loss)\n    train_accuracy = correct / total * 100.0\n    train_accuracies.append(train_accuracy)\n    end_time = time.time()\n    test_time = end_time - start_time\n    print(f\"Training Time: {test_time:.2f} seconds\")\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in valloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Compute loss\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            \n            # Calculate accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss and accuracy\n    val_loss /= len(val_data)\n    val_losses.append(val_loss)\n    val_accuracy = correct / total * 100.0\n    val_accuracies.append(val_accuracy)\n    \n    # Print epoch statistics\n    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, \"\n          f\"Val Accuracy: {val_accuracy:.2f}%\")\n\n# After training, you can evaluate the model on the test data using a similar approach\ntorch.save(model.state_dict(), \"resnet_cbam_weights.pth\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate your ResNet18_CBAM model\nnum_classes = 2  # Change to your number of classes\nmodel = ResNet18_CBAM(num_classes)\n\n# Print the layer names\nfor name, module in model.named_children():\n    print(name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualising the best feature maps--->>keep running code to get the possible indexes\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\n\n# Instantiate your ResNet18_CBAM model\nnum_classes = 2  # Change to your number of classes\nmodel = ResNet18_CBAM(num_classes)\n\n# Load the pretrained weights if needed\n# model.load_state_dict(torch.load('resnet_cbam_weights.pth'))\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Choose the layers from which you want to visualize feature maps\nlayer_names = ['cbam1', 'cbam2','cbam3','cbam4']  # Names of the layers you want to visualize\n\n# Hook function to store the layer's output\ndef hook_fn(module, input, output):\n    feature_maps.append(output.cpu().detach().numpy())\n\n# Collect feature maps\nfeature_maps = []\nhooks = []\nfor layer_name in layer_names:\n    layer = getattr(model, layer_name)\n    hook = layer.register_forward_hook(hook_fn)\n    hooks.append(hook)\n\n# Choose a sample image from the dataset\nsample_image, _ = next(iter(trainloader))\nsample_image = sample_image.to(device)\n\n# Perform a forward pass to collect feature maps\nwith torch.no_grad():\n    model(sample_image)\n\n# Unregister the hooks\nfor hook in hooks:\n    hook.remove()\n\n# Visualize the input image and best feature map\n# Visualize the input image and best feature map\nfor layer_idx, feature_map in enumerate(feature_maps):\n    num_feature_maps = feature_map.shape[1]  # Number of channels\n    \n    # Find the index of the best feature map based on mean activation\n    best_feature_idx = feature_map.mean((0, 1)).argmax()\n    \n    plt.figure(figsize=(3, 3))  # Adjust the figsize if needed\n    \n    # Display the input image\n    plt.subplot(1, 2, 1)\n    plt.imshow(sample_image[0].cpu().numpy().transpose(1, 2, 0))\n    plt.title(\"Input Image\")\n    plt.axis('off')\n    \n    # Display the best feature map\n    plt.subplot(1, 2, 2)\n    \n    # Reshape the feature map for visualization\n    reshaped_feature_map = feature_map[0, best_feature_idx].squeeze()\n    plt.imshow(reshaped_feature_map, cmap='viridis')\n    \n    plt.title(f'Best Feature Map from Layer: {layer_names[layer_idx]}')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}